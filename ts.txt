# -*- coding: utf-8 -*-
"""Final_lab_Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZshQHk3vL1RSjczz4d8RkSY8yf3Lx4ec

!pip install transformers datasets sentencepiece -q

#TEXT SUMMARIZATION
import numpy as np
import pandas as pd
import os
# Disable WandB logging
os.environ["WANDB_DISABLED"] = "true"
from datasets import Dataset
from transformers import T5Tokenizer
from transformers import T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments


#SENTIMENT ANALYSIS
from transformers import pipeline
sentiment_analyzer = pipeline("sentiment-analysis")
import pandas as pd
import re
from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch

#MACHINE TRANSLATION
from transformers import MarianMTModel, MarianTokenizer
from datasets import load_dataset
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments


# **TEXT SUMMARIZATION**

## **With fine tuning - dataset with article and summary**
"""

!pip install transformers datasets

import numpy as np
import pandas as pd
import os
# Disable WandB logging
os.environ["WANDB_DISABLED"] = "true"

"""**TEXT FILE HANDLING - if it is a text file with article and summaries**"""

#
# Read the text file with articles and summaries
# articles = []
# summaries = []

# with open("/content/articles_summaries.txt", "r", encoding="utf-8") as file:
#     for line in file:
#         # Assuming each line contains the article and summary separated by a tab or some other delimiter
#         article, summary = line.strip().split("\t")  # adjust split based on actual delimiter
#         articles.append(article)
#         summaries.append(summary)

# # Create a pandas dataframe from the lists
# import pandas as pd
# df = pd.DataFrame({"article": articles, "summary": summaries})

# # Convert to Hugging Face dataset
# from datasets import Dataset
# dataset = Dataset.from_pandas(df)

"""**TEXT FILE HANDLING - if it is a text file with ONLY ARTICLE**"""

# # Read the text file with articles only
# articles = []

# with open("/content/articles_only.txt", "r", encoding="utf-8") as file:
#     for line in file:
#         articles.append(line.strip())  # Add each article to the list


# # Create a DataFrame from the articles list
# df = pd.DataFrame({"article": articles})

df = pd.read_csv("/content/news_summary.csv", encoding='utf-8', on_bad_lines='skip', engine='python')
df = df[["text", "headlines"]].dropna()
df = df.rename(columns={"text": "article", "headlines": "summary"})

"""**Convert to Hugging Face Dataset & Tokenize**"""

from datasets import Dataset
from transformers import T5Tokenizer

dataset = Dataset.from_pandas(df)

tokenizer = T5Tokenizer.from_pretrained("t5-small")

def preprocess(example):
    input_text = "summarize: " + example["article"]
    target_text = example["summary"]

    input_enc = tokenizer(input_text, max_length=512, padding="max_length", truncation=True)
    target_enc = tokenizer(target_text, max_length=64, padding="max_length", truncation=True)

    input_enc["labels"] = target_enc["input_ids"]
    return input_enc

tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)

"""We are selecting only 100 samples with small_dataset = tokenized_dataset.select(range(100)), which is great for testing, but remember that if we  fine-tune with only 100 samples, the model won't generalize well. It‚Äôs better to fine-tune on a larger subset of the dataset for better results. As it was taking longer time I had 100 samples"""

small_dataset = tokenized_dataset.select(range(100))  # Use 100 samples

"""**Set Up Training (Fine-tuning)**"""

from transformers import T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments

# Load the model
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./t5-finetuned-summary",
    per_device_train_batch_size=4,
    num_train_epochs=1,  # Reduce epochs to 1 for faster training
    logging_dir="./logs",
    logging_steps=100,
    save_strategy="epoch",
    evaluation_strategy="no",
    report_to=None  # Ensure no WandB logging
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=small_dataset,
    tokenizer=tokenizer,
)
trainer.train()

"""**Inference (Test Your Model)**

**On a manual article**
"""

input_text = "summarize: " + df.iloc[37]["article"]
input_ids = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).input_ids

output_ids = model.generate(input_ids, max_length=128, num_beams=8, early_stopping=True)
summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Original:", df.iloc[37]["article"])
print("Expected Summary:", df.iloc[37]["summary"])
print("Generated Summary:", summary)

"""**On new custom article**"""

#new_article = "India launched a new space mission to study the Sun. This mission, called Aditya-L1, will provide data on solar radiation and storms."
# new_article = "Tesla reported record profits this quarter, driven by strong sales of the Model Y and Model 3. CEO Elon Musk highlighted the company's progress in AI and self-driving technology. Tesla also plans to open a new Gigafactory in India next year."
# new_article = "Scientists have discovered a new exoplanet that may be capable of supporting life. Located 120 light-years away, the planet is in the habitable zone of its star and has a similar atmosphere to Earth, with traces of oxygen and water vapor."
new_article = "The World Health Organization has issued a warning about a new strain of the flu virus spreading rapidly in Southeast Asia. Health officials urge people to get vaccinated and practice hygiene to avoid infection during the upcoming flu season."
# new_article = "In a thrilling final match, India won the ICC World Cup by defeating Australia. Virat Kohli scored a century, and Jasprit Bumrah took three crucial wickets. Fans celebrated the victory with parades and fireworks across the country."
# new_article = "Apple has announced the launch of its latest iPhone 15 series. The new iPhones come with an upgraded A17 Bionic chip, improved cameras, and USB-C charging. The Pro models also feature a titanium frame, making them lighter and more durable."

input_text = "summarize: " + new_article
input_ids = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).input_ids

output_ids = model.generate(input_ids, max_length=128, num_beams=8, early_stopping=True)
summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print("Custom Summary:", summary)

"""## **Without fine tuning just take the article column and summarize**"""

df_new = pd.read_csv("/content/news_summary.csv")
df_new = df_new[["text"]].dropna()
df_new = df_new.rename(columns={"text": "article"})

from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

for i in range(5):  # Just first 5 articles for demo
    input_text = "summarize: " + df_new.iloc[i]["article"]
    input_ids = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True).input_ids
    output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    print(f"\nüì∞ Article {i+1}:\n", df_new.iloc[i]["article"])
    print("üìù Generated Summary:\n", summary)

"""# **SENTIMENT ANALYSIS**

## **Pretrained**
"""

#example
from transformers import pipeline
sentiment_analyzer = pipeline("sentiment-analysis")

text = "The movie was absolutely amazing, I loved every part of it!"
result = sentiment_analyzer(text)

print("Text:", text)
print("Sentiment:", result)

text = "The movie was absolutely boring,Disappointed"
result = sentiment_analyzer(text)

print("Text:", text)
print("Sentiment:", result)

#using vaccine dataset, can use artcle also (check for artcle column alone)
from transformers import pipeline
import pandas as pd
import re

# Load the sentiment analysis pipeline
sentiment_analyzer = pipeline("sentiment-analysis")

# Load the dataset
sentiment_df = pd.read_csv("/content/vaccination_tweets.csv")  # Replace with your file path
tweets = sentiment_df['text'].values

# Preprocessing function for cleaning tweets
def data_preprocess(words):
    # Removing any emojis or unknown characters
    words = words.encode('ascii', 'ignore')
    words = words.decode()

    # Splitting string into words
    words = words.split(' ')

    # Removing URLs
    words = [word for word in words if not word.startswith('http')]
    words = ' '.join(words)

    # Removing punctuations
    words = re.sub(r"[^0-9a-zA-Z]+", " ", words)

    # Removing extra spaces
    words = re.sub(' +', ' ', words)
    return words

# Preprocess the tweets
preprocessed_tweets = [data_preprocess(tweet) for tweet in tweets]

# Create a DataFrame with preprocessed tweets
sentiment_df['preprocessed_text'] = preprocessed_tweets

# Take the first 100 rows from the sentiment_df
sample_df = sentiment_df.head(100).copy()

# Apply sentiment analysis to the preprocessed tweets and store results in a new column
sample_df["sentiment"] = [sentiment_analyzer(tweet)[0]['label'] for tweet in sample_df['preprocessed_text']]

# Display the first few results
print(sample_df[["preprocessed_text", "sentiment"]].head(10))

"""## **With fine tuning - sentiment analysis**

Sentiment analysis for fine tuned gives utter bad result as the training size is too small , epoch also. If we increase it is very slow

**SENTIMENT ANALYSIS - SUPPOSE FINE TUNING VARLENA , IPDI model= nu potrunga
sentiment_analyzer = pipeline(
        "sentiment-analysis",
        model="distilbert-base-uncased-finetuned-sst-2-english")**
"""

# TRAINING BLOCK
from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments

# Load IMDb dataset
dataset = load_dataset("imdb")

# Load DistilBERT tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# Tokenize text
def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="no",
    num_train_epochs=0.5,  # quick test
    per_device_train_batch_size=4,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    load_best_model_at_end=False,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].select(range(200)),
    eval_dataset=tokenized_datasets["test"].select(range(100)),
    tokenizer=tokenizer,
)

# Train the model
trainer.train()

# Save the fine-tuned model
trainer.save_model("./results")  # Save for later use

# Evaluate
results = trainer.evaluate()
print("Evaluation results:", results)

# INFERENCE BLOCK
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# Load the fine-tuned model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("./results")  # load trained model

# Inference: Predict sentiment of a new review
def predict_sentiment(text):
    model.eval()  # set model to evaluation mode

    # Tokenize the input text
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True, padding=True)

    # Get model's output (logits)
    with torch.no_grad():  # avoid tracking gradients
        outputs = model(**inputs)

    # Get predicted probabilities (softmax)
    logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=-1)

    # Get predicted label (0 = negative, 1 = positive)
    prediction = torch.argmax(probs, dim=-1).item()

    # Return prediction
    return "positive" if prediction == 1 else "negative"

# Test with some new examples
test_texts = [
    "The movie was fantastic! I really loved it.",
    "It was the worst movie I have ever seen, so boring.",
    "A wonderful experience with great acting and storyline.",
    "Terrible! I would not recommend it to anyone."
]

for text in test_texts:
    sentiment = predict_sentiment(text)
    print(f"Review: {text}")
    print(f"Predicted Sentiment: {sentiment}\n")

"""# **MACHINE TRANSLATION**

## **PRE-TRAINED - MACHINE TRANSLATION**
"""

from transformers import MarianMTModel, MarianTokenizer

def translate(text, src_lang, tgt_lang):
    # Create the model name based on source and target languages
    model_name = f"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}"

    try:
        # Load tokenizer and model
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)
    except Exception as e:
        return f"‚ùå Error loading model for {src_lang} to {tgt_lang}: {e}"

    # Tokenize input text
    encoded = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

    # Generate translation
    translated = model.generate(**encoded)

    # Decode the output
    return tokenizer.decode(translated[0], skip_special_tokens=True)

# üîÑ Interactive use
print("üåê Dynamic Machine Translation")
src = input("Enter source language code (e.g., en, fr, de, es): ").strip()
tgt = input("Enter target language code (e.g., fr, en, de, es): ").strip()
text = input("Enter text to translate:\n")

translation = translate(text, src, tgt)
print(f"\nüì§ Original ({src}): {text}")
print(f"üì• Translated ({tgt}): {translation}")

"""## **FINE TUNING - MACHINE TRANSLATION**"""

!pip install transformers datasets sentencepiece -q

from datasets import load_dataset
from transformers import MarianTokenizer, MarianMTModel, Seq2SeqTrainer, Seq2SeqTrainingArguments

# Load a small en-fr dataset
dataset = load_dataset("opus_books", "en-fr")
small_data = dataset["train"].select(range(100))  # Use just 100 examples

# Load tokenizer and model
model_name = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# Preprocess function
def preprocess(example):
    src = example["translation"]["en"]
    tgt = example["translation"]["fr"]
    model_inputs = tokenizer(src, max_length=128, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(tgt, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize dataset
tokenized = small_data.map(preprocess, batched=False)

# Training arguments (FAST!)
args = Seq2SeqTrainingArguments(
    output_dir="./quick_mt_model",
    per_device_train_batch_size=4,
    num_train_epochs=1,
    save_strategy="no",
    logging_steps=10,
    report_to=None,
    fp16=False  # If on GPU with FP16 support, set True
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=tokenized,
    tokenizer=tokenizer
)

# Train (~2‚Äì4 mins)
trainer.train()

def translate_custom(text):
    input_ids = tokenizer(text, return_tensors="pt", truncation=True, padding=True).input_ids
    output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# Example test
print(translate_custom("The forest was dark and silent."))

# READING CSV OR TEXT FILE

# # Read the text file
# src_sentences = []
# tgt_sentences = []

# with open('text_file.txt', 'r', encoding='utf-8') as file:
#     for line in file:
#         src, tgt = line.strip().split('\t')  # Assuming tab-separated
#         src_sentences.append(src)
#         tgt_sentences.append(tgt)

# # Create a DataFrame and convert to Hugging Face Dataset
# df = pd.DataFrame({"src_text": src_sentences, "tgt_text": tgt_sentences})
# dataset = Dataset.from_pandas(df)


# #READING CSV FILE
# # Load your CSV into a DataFrame
# df = pd.read_csv('csv_file.csv')  # Update with your file path

# # Rename columns for easier access
# df = df.rename(columns={"en": "src_text", "fr": "tgt_text"})

# # Convert DataFrame to Hugging Face Dataset format
# dataset = Dataset.from_pandas(df)